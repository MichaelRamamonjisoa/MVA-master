{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/MVA/NLP/MVA_2018_SL/TD_#3/model/context2vec.ukwac.model.params\n"
     ]
    }
   ],
   "source": [
    "import context2vec.eval as c2v\n",
    "import os\n",
    "MODEL_DIR = '/home/michael/Documents/MVA/NLP/MVA_2018_SL/TD_#3/model/'\n",
    "MODEL_NAME = 'context2vec.ukwac.model.params'\n",
    "PATH = os.path.join(MODEL_DIR,MODEL_NAME)\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy\n",
    "import six\n",
    "import sys\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "from chainer import cuda\n",
    "from context2vec.common.context_models import Toks\n",
    "from context2vec.common.model_reader import ModelReader\n",
    "\n",
    "class ParseException(Exception):\n",
    "    def __init__(self, str):\n",
    "        super(ParseException, self).__init__(str)\n",
    "\n",
    "def parse_input(line):    \n",
    "    target_exp = re.compile('\\[.*\\]')\n",
    "    sent = line.strip().split()\n",
    "    target_pos = None\n",
    "    for i, word in enumerate(sent):\n",
    "        if target_exp.match(word) != None:\n",
    "            target_pos = i\n",
    "            if word == '[]':\n",
    "                word = None\n",
    "            else:\n",
    "                word = word[1:-1]\n",
    "            sent[i] = word\n",
    "    return sent, target_pos    \n",
    "\n",
    "def mult_sim(w, target_v, context_v):\n",
    "    target_similarity = w.dot(target_v)\n",
    "    target_similarity[target_similarity<0] = 0.0\n",
    "    context_similarity = w.dot(context_v)\n",
    "    context_similarity[context_similarity<0] = 0.0\n",
    "    return (target_similarity * context_similarity) \n",
    "\n",
    "def load_c2v_model(model_param_file):\n",
    "        \n",
    "    model_reader = ModelReader(model_param_file)\n",
    "    w = model_reader.w\n",
    "    word2index = model_reader.word2index\n",
    "    index2word = model_reader.index2word\n",
    "    model = model_reader.model\n",
    "    \n",
    "    model_full = [model,w,word2index,index2word]\n",
    "    \n",
    "    return model_full\n",
    "\n",
    "\n",
    "def evalc2v(input_line, c2v_model,verbose=False,n_result=10):\n",
    "    eval_list = []\n",
    "#     n_result = 10  # number of search result to show\n",
    "    gpu = -1 # todo: make this work with gpu\n",
    "    \n",
    "    if gpu >= 0:\n",
    "        cuda.check_cuda_available()\n",
    "        cuda.get_device(gpu).use()    \n",
    "    xp = cuda.cupy if gpu >= 0 else numpy    \n",
    "    \n",
    "    try:\n",
    "        line = input_line\n",
    "        sent, target_pos = parse_input(line)\n",
    "        if target_pos == None:\n",
    "            raise ParseException(\"Can't find the target position.\") \n",
    "                    \n",
    "        if sent[target_pos] == None:\n",
    "            target_v = None\n",
    "        elif sent[target_pos] not in word2index:\n",
    "            raise ParseException(\"Target word is out of vocabulary.\")\n",
    "        else:\n",
    "            target_v = w[word2index[sent[target_pos]]]\n",
    "        if len(sent) > 1:\n",
    "            context_v = c2v_model[0].context2vec(sent, target_pos) \n",
    "            context_v = context_v / xp.sqrt((context_v * context_v).sum())\n",
    "        else:\n",
    "            context_v = None        \n",
    "            \n",
    "        if target_v is not None and context_v is not None:\n",
    "            similarity = mult_sim(c2v_model[1], target_v, context_v)\n",
    "        else:\n",
    "            if target_v is not None:\n",
    "                v = target_v\n",
    "            elif context_v is not None:\n",
    "                v = context_v                \n",
    "            else:\n",
    "                raise ParseException(\"Can't find a target nor context.\")   \n",
    "            similarity = (c2v_model[1].dot(v)+1.0)/2 # Cosine similarity can be negative, mapping similarity to [0,1]\n",
    "                \n",
    "        count = 0\n",
    "        for i in (-similarity).argsort():\n",
    "            if numpy.isnan(similarity[i]):\n",
    "                continue\n",
    "            eval_list.append((c2v_model[3][i], similarity[i]))\n",
    "            \n",
    "            if verbose is True:\n",
    "                print('{0}: {1}'.format(c2v_model[3][i], similarity[i]))\n",
    "            count += 1\n",
    "            if count == n_result:\n",
    "                break\n",
    "\n",
    "    except ParseException as e:\n",
    "        print \"ParseException: {}\".format(e)                \n",
    "    except Exception:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        print \"*** print_tb:\"\n",
    "        traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "        print \"*** print_exception:\"\n",
    "        traceback.print_exception(exc_type, exc_value, exc_traceback, limit=2, file=sys.stdout)\n",
    "    return eval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: /home/michael/Documents/MVA/NLP/MVA_2018_SL/TD_#3/model/context2vec.ukwac.model.params\n",
      "Config:  {'config_path': '/home/michael/Documents/MVA/NLP/MVA_2018_SL/TD_#3/model/', 'model_file': 'context2vec.ukwac.model', 'deep': 'yes', 'drop_ratio': '0.0', 'words_file': 'context2vec.ukwac.words.targets', 'unit': '300'}\n"
     ]
    }
   ],
   "source": [
    "c2v_model = load_c2v_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard-back: 0.551928520203\n",
      "must-read: 0.550950348377\n",
      "32-page: 0.546000599861\n",
      "spiral-bound: 0.544216036797\n",
      "64-page: 0.541346013546\n",
      "coffee-table: 0.539836406708\n",
      "new: 0.538940668106\n",
      "self-published: 0.538060128689\n",
      "48-page: 0.537546575069\n",
      "best-selling: 0.536374986172\n"
     ]
    }
   ],
   "source": [
    "eval_vec = evalc2v('This is a [] book', c2v_model,True,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def computeEditDistance(str1, str2,w_del=1,w_ins=1,w_sub=1):\n",
    "    str1 = str1.lower()\n",
    "    str2 = str2.lower()\n",
    "    m = np.zeros((len(str1)+1,len(str2)+1))\n",
    "    for i in range(len(str1)+1):\n",
    "        m[i,0] = i\n",
    "    for j in range(len(str2)+1):\n",
    "        m[0,j] = j\n",
    "    for i in range(len(str1)+1)[1:]:\n",
    "        for j in range(len(str2)+1)[1:]:\n",
    "            if str1[i-1] == str2[j-1]:\n",
    "#                 m[i,j] = min(m[i-1,j]+w_del, m[i,j-1]+w_ins, m[i-1,j-1])\n",
    "                m[i,j] = m[i-1,j-1]\n",
    "            else:\n",
    "                m[i,j] = min(m[i-1,j]+w_del, m[i,j-1]+w_ins, m[i-1,j-1]+w_sub)\n",
    "    return m[len(str1),len(str2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we change the weights, the Edit distance function is not symmetric anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the corpus, word dictionnary, slang dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'Corpus/CorpusBataclan_en.1M.raw.txt'\n",
    "text = []\n",
    "def read_dataset(dataset):\n",
    "    text = []\n",
    "    with open(dataset, \"r\") as f:\n",
    "        text = f.readlines()\n",
    "        \n",
    "    for i,line in enumerate(text):\n",
    "        text[i] = line.split('\\n')[0]\n",
    "    return text\n",
    "\n",
    "def write_output(filename,text_list):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for tweet in text_list:\n",
    "            f.write(tweet+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_full = read_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slang dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_abbreviation = {'a':{'atm':'at the moment',\n",
    "                          'af':'as fuck',\n",
    "                          'afk':'away from keyboard'},\n",
    "                     'b':{'b':'be',\n",
    "                          'bc':'because',\n",
    "                          'brb':'be right back',\n",
    "                          'bro':'friend',\n",
    "                          'bb':'baby',\n",
    "                          'bm':'bad manners',\n",
    "                          'bs':'bullshit',\n",
    "                          'bbq':'barbecue'},\n",
    "                     'c':{'cya':'see you later',\n",
    "                          'cu':'see you later',\n",
    "                          'cus':'because',\n",
    "                         },\n",
    "                     'd':{'dis':'this',\n",
    "                          'dat':'that',\n",
    "                          'dawg':'friend',\n",
    "                          'dafuq':'what the fuck',\n",
    "                          'dm':'direct message',\n",
    "                          'dang':'wow'},\n",
    "                     'e':{'ez':'easy'\n",
    "                         },\n",
    "                     'f':{'fu':'fuck you',\n",
    "                          'ffs':'for fuck sake',\n",
    "                          'fr':'for real',\n",
    "                          'fml':'fuck my life'},\n",
    "                     'g':{'gov':'government',\n",
    "                          'gg':'good game',\n",
    "                          'ggla':'good game, love all',\n",
    "                          'gr8':'great',\n",
    "                          'gtfo':'go away',\n",
    "                          'gth':'go to hell',\n",
    "                          'gtg':'have to go'\n",
    "                         },\n",
    "                     'h':{},\n",
    "                     'i':{'ikr':'I know, right?',\n",
    "                          'idc':'I do not care',\n",
    "                          'idgaf':'I do not care',\n",
    "                          'immo':'in my modest opinion',\n",
    "                          'iirc':'if I remember correctly'\n",
    "                         },\n",
    "                     'j':{},\n",
    "                     'k':{'k':'okay',\n",
    "                          'kk':'okay'\n",
    "                         },\n",
    "                     'l':{'lmao':'*laugh',\n",
    "                          'lmfao':'*laugh*',\n",
    "                          'lol':'*laugh*'\n",
    "                         },\n",
    "                     'm':{'m8':'mate',\n",
    "                          'mf':'motherfucker'\n",
    "                         },\n",
    "                     'n':{'nsfw':'not safe for work',\n",
    "                          'nbd':'no big deal'},\n",
    "                     'o':{'ok':'okay',\n",
    "                          'omg':'oh my god',\n",
    "                          'omfg':'oh my fucking god'},\n",
    "                     'p':{'pm':'private message',\n",
    "                          'ppl':'people',\n",
    "                          'plz':'please'\n",
    "                         },\n",
    "                     'q':{},\n",
    "                     'r':{'r':'are',\n",
    "                          'rofl':'*laugh*',\n",
    "                          'rn':'right now',\n",
    "                         'ru':'are you'},\n",
    "                     's':{'smh':'shaking my head',\n",
    "                          'sis':'friend',\n",
    "                          'sry':'sorry',\n",
    "                          'stfu':'shut the fuck up',\n",
    "                          'smth':'something'\n",
    "                         },\n",
    "                     't':{'ttyl':'talk to you later',\n",
    "                          'tn':'tonight'},\n",
    "                     'u':{'u':'you',\n",
    "                          'ur':'your'},\n",
    "                     'v':{},\n",
    "                     'w':{'wtf':'what the fuck',\n",
    "                          'wth':'what the hell'},\n",
    "                     'x':{'xoxo':'kissing and hugging',\n",
    "                          'xo':'kissing',\n",
    "                          'xx':'kissing'},\n",
    "                     'y':{'yolo':'you only live once'},\n",
    "                     'z':{}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import word dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abcTo123():\n",
    "    alphabet_dict = {'a':0,\n",
    "                 'b':1,\n",
    "                 'c':2,\n",
    "                 'd':3,\n",
    "                 'e':4,\n",
    "                 'f':5,\n",
    "                 'g':6,\n",
    "                 'h':7,\n",
    "                 'i':8,\n",
    "                 'j':9,\n",
    "                 'k':10,\n",
    "                 'l':11,\n",
    "                 'm':12,\n",
    "                 'n':13,\n",
    "                 'o':14,\n",
    "                 'p':15,\n",
    "                 'q':16,\n",
    "                 'r':17,\n",
    "                 's':18,\n",
    "                 't':19,\n",
    "                 'u':20,\n",
    "                 'v':21,\n",
    "                 'w':22,\n",
    "                 'x':23,\n",
    "                 'y':24,\n",
    "                 'z':25}\n",
    "    \n",
    "    return alphabet_dict\n",
    "\n",
    "def build_indexed_alphabet(word_dict_txt):\n",
    "    large_dict = []\n",
    "    alphabet = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "    \n",
    "    for letter in alphabet:\n",
    "        list_letter = [word for word in word_dict_txt if word.startswith(letter)]\n",
    "        large_dict.append(list_letter)\n",
    "        \n",
    "    return large_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "large_dict_txtfile = read_dataset('large.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "large_dict = build_indexed_alphabet(large_dict_txtfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all tweet normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing non ASCII elements\n",
    "\n",
    "def remove_no_ascii(text):\n",
    "    import re\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "    text = re.sub(r'&amp;',r'and',text)\n",
    "    text = re.sub(r'&gt;',r'>',text)\n",
    "    text = re.sub(r'&lt;',r'<',text)\n",
    "    text = re.sub(r'\\'',r\"'\",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't do drugs and alcohol\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Don\\'t do drugs &amp; alcohol'\n",
    "remove_no_ascii(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing RTs\n",
    "\n",
    "def clear_RTs(text):\n",
    "            \n",
    "    clean_text = text\n",
    "    idx = clean_text.find('RT')\n",
    "\n",
    "    while idx != -1:    \n",
    "        idx_end = clean_text.find(': ',idx)+1\n",
    "        if idx_end == 0:\n",
    "            idx_end = len(clean_text)\n",
    "        RT = clean_text[idx:idx_end]\n",
    "        clean_text = clean_text.replace(RT, '')\n",
    "        idx = clean_text.find('RT')\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_At(text):\n",
    "    \n",
    "    clean_text = re.sub(r'@[a-zA-Z0-9_]{1,15}','',text)               \n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing HTML\n",
    "\n",
    "def clear_HTMLs(text):\n",
    "    \n",
    "    text = re.sub(r'<[a-zA-Z0-9]>','',text)\n",
    "    text = re.sub(r'</[a-zA-Z0-9]>','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facebook'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '<b>Facebook</b>'\n",
    "clear_HTMLs(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseHashtags(text):\n",
    "            \n",
    "    clean_text = text\n",
    "    idx = clean_text.find('#')\n",
    "    \n",
    "    while idx != -1:\n",
    "        idx_end = 1\n",
    "        if idx!=len(clean_text):\n",
    "            while idx+idx_end<len(clean_text):\n",
    "                if not clean_text[idx+idx_end].isalpha() and not clean_text[idx+idx_end].isdigit():\n",
    "                    break\n",
    "                else:\n",
    "                    idx_end += 1\n",
    "                    \n",
    "            hashtag = clean_text[idx:idx+idx_end]\n",
    "#             print(hashtag)\n",
    "            hashtag = re.sub(r'#([a-zA-Z0-9])',r' #\\1',hashtag)   \n",
    "#             print(hashtag)\n",
    "            hashtag = re.sub(r'([a-zA-Z])([0-9])([a-zA-Z])',r'\\1-\\2-\\3',hashtag)\n",
    "#             print(hashtag)\n",
    "            hashtag = re.sub(r'([0-9])([a-zA-Z])',r'\\1-\\2',hashtag)     \n",
    "#             print(hashtag)            \n",
    "            hashtag = re.sub(r'([a-z])([A-Z0-9])',r'\\1-\\2',hashtag)\n",
    "#             print(hashtag)\n",
    "            hashtag = re.sub(r'([A-Z]{1})([A-Z]{1})([a-z0-9])',r'\\1-\\2\\3',hashtag)\n",
    "\n",
    "        clean_text = clean_text[:idx]+hashtag+clean_text[idx+idx_end:]\n",
    "        idx = clean_text.find('#',idx+len(hashtag))\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey  #Pray-4-Paris #Hello #world #4-Paris #ELLIS-Story #w-ORLD for #4-Paris'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'hey #Pray4Paris#Hello#world#4Paris#ELLISStory#wORLD for#4Paris'\n",
    "parseHashtags(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correctSlang(text,dictionnary):\n",
    "    clean_text = text\n",
    "    split = re.split(\"-|;|:|\\(|\\)|\\[|\\]|\\.| |,|!|\\~|\\?\",text)\n",
    "    parsed_tweet = [s for s in split if len(s)!=0]\n",
    "    \n",
    "    for word in parsed_tweet:#re.findall(r\"[\\w']+\", text):\n",
    "        word = word.lower()\n",
    "        idx = 0\n",
    "        if len(word)==1 and word[0].isalpha():\n",
    "            if word in dictionnary[word.lower()]:\n",
    "                idx = clean_text.lower().find(word,idx)\n",
    "                while idx!=-1:                    \n",
    "                    if idx<len(clean_text)-1:\n",
    "                        if clean_text[idx-1].isalpha() or clean_text[idx+1].isalpha():\n",
    "                            idx = clean_text.lower().find(word,idx+1)\n",
    "                        else:\n",
    "                            clean_text = clean_text[:idx]+dictionnary[word[0].lower()][word]+clean_text[idx+1:]\n",
    "                            idx = clean_text.lower().find(word,idx+len(dictionnary[word[0].lower()][word]))\n",
    "                    elif idx==len(clean_text)-1:\n",
    "                        if not clean_text[idx-1].isalpha():\n",
    "                            clean_text = clean_text[:idx]+dictionnary[word[0].lower()][word]\n",
    "                        idx=-1\n",
    "                        \n",
    "                    elif idx==0:\n",
    "                        if not clean_text[idx+1].isalpha():\n",
    "                            clean_text = dictionnary[word.lower()][word]+clean_text[1:]\n",
    "                            idx = clean_text.lower().find(word,idx+len(dictionnary[word.lower()][word]))\n",
    "                    \n",
    "                idx = 0\n",
    "        \n",
    "        elif len(word)>1 and word[0].isalpha():\n",
    "            if word in dictionnary[word[0].lower()]:\n",
    "                idx = clean_text.lower().find(word,idx)\n",
    "                while idx!=-1:                     \n",
    "                    if idx>0:                        \n",
    "                        if idx<len(clean_text)-len(word):\n",
    "                            if clean_text[idx-1].isalpha() or clean_text[idx+len(word)].isalpha():\n",
    "                                idx = clean_text.lower().find(word,idx+1)\n",
    "                                \n",
    "                            else:\n",
    "                                clean_text = clean_text[:idx]+dictionnary[word[0].lower()][word]+clean_text[idx+len(word):]\n",
    "                                idx = clean_text.lower().find(word,idx+len(dictionnary[word[0].lower()][word]))\n",
    "                                \n",
    "                        elif idx==len(clean_text)-len(word):\n",
    "                            if not clean_text[idx-1].isalpha():\n",
    "                                clean_text = clean_text[:idx]+dictionnary[word[0].lower()][word]\n",
    "                            idx=-1\n",
    "                                \n",
    "                    elif idx==0:\n",
    "                        if idx+len(word)<len(clean_text):\n",
    "                            if not clean_text[idx+len(word)].isalpha():\n",
    "                                clean_text = dictionnary[word[0].lower()][word]+clean_text[len(word):]\n",
    "                            idx = clean_text.lower().find(word,idx+len(dictionnary[word[0].lower()][word]))\n",
    "                        else:\n",
    "                            idx=-1\n",
    "                idx = 0\n",
    "                \n",
    "    return clean_text\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sorry have to go, be right back !  what the fuck??? \"GTfO and shut the fuck up for fuck sake!\"... are you kidding me ? are you freaking for real? fuck you mate! *laugh* *laugh* *laugh* fuck my life, I know, right? you only live once right now mate,shaking my head'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'SRY GtG, Brb !  WTF??? \"GTfO and stfu ffs!\"... R U kidding me ? r u freaking fR? fu m8! LMFAO ROFL LoL FmL, iKr YoLo rn m8,smh'\n",
    "correctSlang(a,dict_abbreviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_ends(text):\n",
    "    clean_text = ''\n",
    "    j = 0\n",
    "    while j<len(text) and text[j].isalpha() is False:\n",
    "        j=j+1\n",
    "    clean_text = text[j:]\n",
    "    \n",
    "    j=1\n",
    "    while True:\n",
    "        if len(clean_text)>1:\n",
    "            if not clean_text[-j].isalpha() and not clean_text[-j-1].isalpha():\n",
    "                clean_text = clean_text[:-j-1]\n",
    "            elif clean_text[-j].isalpha() and clean_text[-j-1].isalpha():\n",
    "                break\n",
    "            else:\n",
    "                clean_text = clean_text[:-j]\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "#     if clean_text[-1].isalpha()\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "hahlaalalojjh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ' ?1. hahlaalalojjh j,.:; ;   .  '\n",
    "print(len(a))\n",
    "print(remove_ends(a))\n",
    "len(remove_ends(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_URLs(text):\n",
    "    clean_text = text\n",
    "    idx = clean_text.find('http')\n",
    "\n",
    "    while idx != -1:    \n",
    "        idx_end = clean_text.find(' ',idx)\n",
    "        if idx_end == -1:\n",
    "            idx_end = len(clean_text)\n",
    "        url = clean_text[idx:idx_end]\n",
    "        clean_text = clean_text.replace(url, '')\n",
    "        idx = clean_text.find('http')\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click on this link: '"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'click on this link: https://www.thelink.com/'\n",
    "clear_URLs(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_verb_contractions(text):\n",
    "\n",
    "    clean_text = text\n",
    "    \n",
    "    idx = clean_text.lower().find(\"'s\")\n",
    "    while idx != -1 and idx!=1:\n",
    "        pre_letter = clean_text[idx-2]+clean_text[idx-1]\n",
    "        if idx<len(clean_text)-len(\"'s\"):\n",
    "            post_letter = clean_text[idx+len(\"'s\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "            \n",
    "        if pre_letter.lower() in ['he','it','at'] and not post_letter.isalpha():\n",
    "            \n",
    "            idx_end = idx+len(\"'s\")                \n",
    "            word = clean_text[idx:idx_end]\n",
    "            if word.upper()==word:\n",
    "                clean_text = clean_text[:idx] + \" 'S \" + clean_text[idx+len(\"'s\")+1:]#clean_text[idx:].replace(word, \" 'S\", 1)\n",
    "            else:\n",
    "                clean_text = clean_text[:idx] + \" 's \" + clean_text[idx+len(\"'s\")+1:]\n",
    "        idx = clean_text.lower().find(\"'s\",idx+1,len(clean_text))\n",
    "        \n",
    "    idx = clean_text.lower().find(\"'d\")\n",
    "    while idx != -1 and idx!=1:\n",
    "        pre_letter = clean_text[idx-2]+clean_text[idx-1]\n",
    "        if idx<len(clean_text)-len(\"'d\"):\n",
    "            post_letter = clean_text[idx+len(\"'d\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "            \n",
    "        if pre_letter.lower() in ['he','it','at','ou'] and not post_letter.isalpha():\n",
    "            \n",
    "            idx_end = idx+len(\"'d\")                \n",
    "            word = clean_text[idx:idx_end]\n",
    "            if word.upper()==word:\n",
    "                clean_text = clean_text[:idx] + \" 'D \" + clean_text[idx+len(\"'D\")+1:]#clean_text[idx:].replace(word, \" 'S\", 1)\n",
    "            else:\n",
    "                clean_text = clean_text[:idx] + \" 'd \" + clean_text[idx+len(\"'d\")+1:]\n",
    "        idx = clean_text.lower().find(\"'d\",idx+1,len(clean_text))\n",
    "    \n",
    "    idx = clean_text.lower().find(\"'re\")\n",
    "    while idx != -1 and idx!=1:\n",
    "        pre_letter = clean_text[idx-1]\n",
    "        if idx<len(clean_text)-len(\"'re\"):\n",
    "            post_letter = clean_text[idx+len(\"'re\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "        if pre_letter.lower() in ['u','y','e'] and not post_letter.isalpha():\n",
    "            \n",
    "            idx_end = idx+len(\"'re\")                \n",
    "            word = clean_text[idx:idx_end]\n",
    "            if word.upper()==word:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, ' ARE', 1)\n",
    "            else:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, ' are', 1)\n",
    "\n",
    "        idx = clean_text.lower().find(\"'re\",idx+1,len(clean_text))\n",
    "        \n",
    "    idx = clean_text.lower().find(\"'ve\")\n",
    "    while idx != -1 and idx!=1:\n",
    "        pre_letter = clean_text[idx-1]\n",
    "        if idx<len(clean_text)-len(\"'ve\"):\n",
    "            post_letter = clean_text[idx+len(\"'ve\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "        if pre_letter.lower() in ['i','u','y','e','t'] and not post_letter.isalpha():\n",
    "             \n",
    "            idx_end = idx+len(\"'ve\")                \n",
    "            word = clean_text[idx:idx_end]\n",
    "            if word.upper()==word:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, ' HAVE', 1)\n",
    "            else:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, ' have', 1)\n",
    "\n",
    "        idx = clean_text.lower().find(\"'ve\",idx+1,len(clean_text))\n",
    "        \n",
    "    idx = clean_text.lower().find(\"'ll\")\n",
    "    while idx != -1 and idx!=1:\n",
    "        pre_letter = clean_text[idx-1]\n",
    "        if idx<len(clean_text)-len(\"'ll\"):\n",
    "            post_letter = clean_text[idx+len(\"'ll\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "        if pre_letter.lower() in ['i','u','y','e','t'] and not post_letter.isalpha():\n",
    "            \n",
    "            idx_end = idx+len(\"'ll\")            \n",
    "            word = clean_text[idx:idx_end]\n",
    "            \n",
    "            if word.upper()==word:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, ' WILL', 1)\n",
    "            else:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, ' will', 1)\n",
    "\n",
    "        idx = clean_text.lower().find(\"'ll\",idx+1,len(clean_text))\n",
    "    \n",
    "        \n",
    "    idx = clean_text.lower().find(\"n't\")\n",
    "    while idx != -1 and idx!=1:\n",
    "        pre_letter = clean_text[idx-1]\n",
    "        if idx<len(clean_text)-len(\"n't\"):\n",
    "            post_letter = clean_text[idx+len(\"n't\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "        if pre_letter.lower() in ['o','s','a','e','d'] and not post_letter.isalpha():\n",
    "        \n",
    "            idx_end = idx+len(\"n't\")    \n",
    "            word = clean_text[idx:idx_end]\n",
    "            \n",
    "            if clean_text.lower()[idx-1] != 'a':\n",
    "                if word.upper()==word:\n",
    "                    clean_text = clean_text[:idx] + clean_text[idx:].replace(word, ' NOT', 1)\n",
    "                else:\n",
    "                    clean_text = clean_text[:idx] + clean_text[idx:].replace(word, ' not', 1)\n",
    "            else:\n",
    "                if word.upper()==word:\n",
    "                    clean_text = clean_text[:idx] + clean_text[idx:].replace(word, 'NNOT', 1)\n",
    "                else:\n",
    "                    clean_text = clean_text[:idx] + clean_text[idx:].replace(word, 'nnot', 1)\n",
    "            \n",
    "        idx = clean_text.lower().find(\"n't\",idx+1,len(clean_text))\n",
    "        \n",
    "    idx = clean_text.lower().find(\"gonna\")\n",
    "    while idx != -1:\n",
    "        if idx!=1:\n",
    "            pre_letter = clean_text[idx-1]\n",
    "        else:\n",
    "            pre_letter = '.'\n",
    "        if idx<len(clean_text)-len(\"gonna\"):\n",
    "            post_letter = clean_text[idx+len(\"gonna\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "        if not pre_letter.isalpha() and not post_letter.isalpha():\n",
    "            \n",
    "            idx_end = idx+len(\"gonna\")            \n",
    "            word = clean_text[idx:idx_end]\n",
    "            \n",
    "            if word.upper()==word:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, 'GOING TO', 1)\n",
    "            else:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, 'going to', 1)\n",
    "\n",
    "        idx = clean_text.lower().find(\"gonna\",idx+1,len(clean_text))    \n",
    "        \n",
    "        \n",
    "    idx = clean_text.lower().find(\"wanna\")\n",
    "    while idx != -1:\n",
    "        if idx!=1:\n",
    "            pre_letter = clean_text[idx-1]\n",
    "        else:\n",
    "            pre_letter = '.'\n",
    "        if idx<len(clean_text)-len(\"wanna\"):\n",
    "            post_letter = clean_text[idx+len(\"wanna\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "        if not pre_letter.isalpha() and not post_letter.isalpha():\n",
    "            \n",
    "            idx_end = idx+len(\"wanna\")            \n",
    "            word = clean_text[idx:idx_end]\n",
    "            \n",
    "            if word.upper()==word:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, 'WANT TO', 1)\n",
    "            else:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, 'want to', 1)\n",
    "\n",
    "        idx = clean_text.lower().find(\"wanna\",idx+1,len(clean_text))  \n",
    "        \n",
    "    idx = clean_text.lower().find(\"gotta\")\n",
    "    while idx != -1:\n",
    "        if idx!=1:\n",
    "            pre_letter = clean_text[idx-1]\n",
    "        else:\n",
    "            pre_letter = '.'\n",
    "        if idx<len(clean_text)-len(\"gotta\"):\n",
    "            post_letter = clean_text[idx+len(\"gotta\")]\n",
    "        else:\n",
    "            post_letter = '.'\n",
    "        if not pre_letter.isalpha() and not post_letter.isalpha():\n",
    "            \n",
    "            idx_end = idx+len(\"gotta\")            \n",
    "            word = clean_text[idx:idx_end]\n",
    "            \n",
    "            if word.upper()==word:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, 'GOT TO', 1)\n",
    "            else:\n",
    "                clean_text = clean_text[:idx] + clean_text[idx:].replace(word, 'got to', 1)\n",
    "\n",
    "        idx = clean_text.lower().find(\"gotta\",idx+1,len(clean_text))  \n",
    "            \n",
    "    clean_text = re.sub(r'([iI]{1})\\'[Mm]',r'\\1 am', clean_text)\n",
    "    clean_text = re.sub(r'I\\'M',r'I AM', clean_text)\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the apostrophes in it's, she's, he's to later tokenize \"'s\" and replace it with context2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It does not DOES NOT cannot i am it 's CANNOT they are THEY ARE i have that will I have I HAVE you will YOU will YOU WILL does not i will that 's \""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"It doesn't DOESN'T can't i'M it's CAN'T they're THEY'RE i've that'll I've I'VE you'll YOU'll YOU'LL doesn't i'll that's\"\n",
    "remove_verb_contractions(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using context2vec and Levenshtein distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is the most complicated one as it corrects misspelled words:\n",
    "- it first loads a word dictionnary, with words indexed by their first letter <b>indexed_word_dict</b> \n",
    "- it then parses the tweet in seperate words\n",
    "- for each word in the tweet:\n",
    "    - check if it exists in <b>indexed_word_dict</b> \n",
    "    - if it does, keep it as such\n",
    "    - if it does not, and is not a proper noun (first letter being a capital letter), correct it using context2vec\n",
    "        - make a list of candidates using both <b>nb_c2v</b> context2vec outputs and words starting with the same letter in <b>indexed_word_dict</b>\n",
    "        - choose the candidate with smallest <b>Levenshtein distance</b> to the original word (a weighing of the candidates is done based on their probability given by context2vec)\n",
    "        - as some tweets are in the middle of a sentence, last words tend to be cut, therefore we put less weight on letter insertion than substitution or deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def use_dict2vec(tweet, c2v_model,indexed_word_dict,w_del=2,w_ins=0.5,w_sub=1,nb_c2v=100,verbose=False):\n",
    "    split = re.split(\"-|;|:|\\(|\\)|\\[|\\]|\\.| |,|!|\\~|\\?\",tweet)\n",
    "    parsed_tweet = [s for s in split if len(s)!=0]\n",
    "#     re.findall(r\"[#\\w\\'*]+\", tweet)\n",
    "    alphabet_dict = abcTo123()\n",
    "    if len(parsed_tweet)>1:\n",
    "        for word in parsed_tweet:\n",
    "            if word[0].isalpha() and word[0].islower():\n",
    "                dict_letter = indexed_word_dict[alphabet_dict[word[0].lower()]]\n",
    "                \n",
    "                if word.lower() not in dict_letter:\n",
    "                    i_start = 0\n",
    "                    w = [w_del,w_ins,w_sub]\n",
    "                    stop = False\n",
    "                    while stop is False:                        \n",
    "                        idx = tweet.find(word,i_start)\n",
    "                        if idx>0 and idx<len(tweet)-len(word):\n",
    "                            if not tweet[idx-1].isalpha() and not tweet[idx+len(word)].isalpha():\n",
    "                                tweet2pred = tweet[:idx].lower()+' [] '+tweet[idx+len(word):].lower()\n",
    "                                eval_vec = evalc2v(tweet2pred,c2v_model,False,nb_c2v)\n",
    "                                stop=True\n",
    "                            else:\n",
    "                                i_start = idx+1\n",
    "                        elif idx==0 and len(tweet)>1:\n",
    "                            if not tweet[idx+len(word)].isalpha():\n",
    "                                tweet2pred = tweet[:idx].lower()+' [] '+tweet[idx+len(word):].lower()\n",
    "                                # we put everything on lower case to use it with c2v\n",
    "                                eval_vec = evalc2v(tweet2pred,c2v_model,False,nb_c2v)\n",
    "                                stop=True\n",
    "                            else:\n",
    "                                i_start = idx+1\n",
    "                                \n",
    "                        elif idx==len(tweet)-len(word):\n",
    "                            if not tweet[idx-1].isalpha():\n",
    "                                tweet2pred = tweet[:idx].lower()+' [] '+tweet[idx+len(word):].lower()\n",
    "                                eval_vec = evalc2v(tweet2pred,c2v_model,False,nb_c2v)\n",
    "                                w = [2,1,1]\n",
    "                                stop=True\n",
    "                            else:\n",
    "                                i_start = idx+1\n",
    "                        else:\n",
    "                            i_start = idx+1\n",
    "                            \n",
    "                    if verbose is True:\n",
    "                        print(tweet2pred)\n",
    "                    dist = 100\n",
    "                    idx_min = 0\n",
    "                    \n",
    "                    candidates = [e[0] for e in eval_vec if e[0].isalpha()]+dict_letter\n",
    "                                        \n",
    "                    for i,candidate in enumerate(candidates):\n",
    "                        if candidate[0].isalpha():\n",
    "                            dist_temp = computeEditDistance(word.lower(), candidate.lower(),w[0],w[1],w[2])\n",
    "                            if i<len(eval_vec):\n",
    "                                dist_temp = dist_temp*(1/eval_vec[i][1])*0.4\n",
    "                                # This lowers the distance of words predicted with context2vec, with most probable words having closest distance\n",
    "                                # if the prediction has a probability <0.4, the distance is then amplified by a factor 0.4/probability\n",
    "#                             computeEditDistance(str1, str2,w_del=1,w_ins=1,w_sub=1):\n",
    "                            if dist_temp<dist:\n",
    "                                dist=dist_temp\n",
    "                                idx_min = i\n",
    "                \n",
    "                    tweet = tweet[:idx]+candidates[idx_min]+tweet[idx+len(word):]\n",
    "                    \n",
    "            elif word.lower()==\"'s\" or word.lower()==\"'d\":\n",
    "                i_start = 0\n",
    "                stop = False\n",
    "                while stop is False:\n",
    "                    idx = tweet.find(word,i_start)\n",
    "                    if idx>0 and idx<len(tweet)-len(word):\n",
    "                        if not tweet[idx-1].isalpha() and not tweet[idx+len(word)].isalpha():\n",
    "                            tweet2pred = tweet[:idx].lower()+' [] '+tweet[idx+len(word):].lower()\n",
    "                            eval_vec = evalc2v(tweet2pred,c2v_model,False,10)\n",
    "                            stop=True\n",
    "                        else:\n",
    "                            i_start = idx+1\n",
    "                    elif idx==0 and len(tweet)>1:\n",
    "                        if not tweet[idx+len(word)].isalpha():\n",
    "                            tweet2pred = tweet[:idx].lower()+' [] '+tweet[idx+len(word):].lower()\n",
    "                            eval_vec = evalc2v(tweet2pred,c2v_model,False,10)\n",
    "                            stop=True\n",
    "                        else:\n",
    "                            i_start = idx+1\n",
    "                            \n",
    "                    elif idx==len(tweet)-len(word):\n",
    "                        if not tweet[idx-1].isalpha():\n",
    "                            tweet2pred = tweet[:idx].lower()+' [] '+tweet[idx+len(word):].lower()\n",
    "                            eval_vec = evalc2v(tweet2pred,c2v_model,False,10)\n",
    "                            \n",
    "                            stop=True\n",
    "                        else:\n",
    "                            i_start = idx+1\n",
    "                    else:\n",
    "                        i_start = idx+1\n",
    "                        \n",
    "                if verbose is True:\n",
    "                    print(tweet2pred)\n",
    "                dist = 100\n",
    "                idx_min = 0\n",
    "                \n",
    "                candidates = [e[0] for e in eval_vec if e[0].isalpha()]\n",
    "                for i,candidate in enumerate(candidates):\n",
    "                    if candidate[0].isalpha():\n",
    "                        dist_temp = computeEditDistance(word.lower(),candidate.lower(),1000,1,1000)\n",
    "                        \n",
    "                        if dist_temp<dist and dist_temp<5:\n",
    "                            dist=dist_temp\n",
    "                            idx_min = i\n",
    "                            \n",
    "                tweet = tweet[:idx]+candidates[idx_min]+tweet[idx+len(word):]\n",
    "\n",
    "                \n",
    "    else:\n",
    "        w = [2,1,2]\n",
    "        if tweet[0].isalpha() and tweet[0].islower():\n",
    "            dict_letter = large_dict[alphabet_dict[tweet[0].lower()]]\n",
    "            if tweet not in dict_letter:\n",
    "                dist = 100\n",
    "                idx_min = 0\n",
    "                for i, candidate in enumerate(dict_letter):\n",
    "                    dist_temp = computeEditDistance(tweet.lower(),candidate.lower(),w[0],w[1],w[2])\n",
    "                    if dist_temp<dist:\n",
    "                        dist=dist_temp\n",
    "                        idx_min = i\n",
    "                        \n",
    "                tweet = dict_letter[idx_min]\n",
    "            \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I personaly think it wud be easier like this, but I could be wrong.\n"
     ]
    }
   ],
   "source": [
    "a = \"I persolany think it 'd be easier like this, but I ccould be wrng.\"\n",
    "b = use_dict2vec(a, c2v_model,large_dict,1,1,1,400)\n",
    "\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_homophones(tweet,c2v_model):\n",
    "#     parsed_tweet = re.findall(r\"[#\\w\\']+\", tweet)\n",
    "    split = split = re.split(\"-|;|:|\\(|\\)|\\[|\\]|\\.| |,|!|\\~|\\?\",tweet)\n",
    "    parsed_tweet = [s for s in split if len(s)!=0]\n",
    "    idx_start = 0\n",
    "    \n",
    "    word = parsed_tweet[0]\n",
    "    if word[0].islower():\n",
    "        tweet2pred = ' [] '+tweet[len(parsed_tweet[0]):]\n",
    "        eval_vec = evalc2v(tweet2pred,c2v_model,False,4)\n",
    "        dist = 100\n",
    "        idx_min = -1\n",
    "        candidates = [ev[0] for ev in eval_vec if ev[0].isalpha()]\n",
    "        for i,e in enumerate(candidates):\n",
    "            dist_temp = computeEditDistance(word.lower(), e,1,1,1)\n",
    "            if dist_temp<3 and dist_temp<dist:\n",
    "                dist=dist_temp\n",
    "                idx_min = i\n",
    "                \n",
    "        if idx_min != -1:          \n",
    "            tweet = candidates[idx_min]+' '+tweet[len(word):]    \n",
    "            idx_start += len(candidates[idx_min])+1\n",
    "            \n",
    "        idx_start = len(word)+1\n",
    "    else:\n",
    "        idx_start = len(word)+1\n",
    "        \n",
    "    for word in parsed_tweet[1:]:\n",
    "        if word[0].islower():\n",
    "            idx_start_updated = tweet.find(word, idx_start)\n",
    "            tweet2pred = tweet[:idx_start_updated]+' [] '+tweet[idx_start_updated+len(word):]\n",
    "            eval_vec = evalc2v(tweet2pred,c2v_model,False,4)\n",
    "            dist = 100\n",
    "            idx_min = -1\n",
    "            candidates = [ev[0] for ev in eval_vec if ev[0].isalpha()]\n",
    "            for i,e in enumerate(candidates):\n",
    "                dist_temp = computeEditDistance(word.lower(), e,1,1,1)\n",
    "                if dist_temp<3 and dist_temp<dist:\n",
    "                    dist=dist_temp\n",
    "                    idx_min = i\n",
    "                    \n",
    "            if idx_min !=-1:               \n",
    "                tweet = tweet[:idx_start_updated]+candidates[idx_min]+tweet[idx_start_updated+len(word):]  \n",
    "                idx_start += len(candidates[idx_min])+1\n",
    "            else:\n",
    "                idx_start += len(word)+1\n",
    "        else:\n",
    "            idx_start += len(word)+1\n",
    "                       \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know where to go'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"I do not know were to go\"\n",
    "clean_homophones(tweet,c2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_tweet_text(text,c2v_model,wordDict,slangDict,verbose=False):\n",
    "    \n",
    "    normalized_text = []\n",
    "    tASCII = 0\n",
    "    tRT = 0\n",
    "    tURLs = 0\n",
    "    tHash = 0\n",
    "    tAt = 0\n",
    "    tEnds = 0\n",
    "    tVerb = 0\n",
    "    tSlang = 0\n",
    "    tD2V = 0\n",
    "    tHomo = 0\n",
    "    \n",
    "    for i,tweet in enumerate(text):\n",
    "        if len(tweet)!=0:  \n",
    "            t = time.time()\n",
    "            tweet = remove_no_ascii(tweet)\n",
    "            tASCII = tASCII + time.time()-t\n",
    "            \n",
    "            t = time.time()\n",
    "            tweet = clear_RTs(tweet)\n",
    "            tRT = tRT + time.time()-t\n",
    "            \n",
    "            t = time.time()\n",
    "            tweet = clear_URLs(tweet)\n",
    "            tweet = clear_HTMLs(tweet)\n",
    "            tURLs = tURLs + time.time()-t\n",
    "            \n",
    "            t = time.time()\n",
    "            tweet = parseHashtags(tweet)\n",
    "            tHash = tHash + time.time()-t\n",
    "            \n",
    "            t = time.time()\n",
    "            tweet = clear_At(tweet)\n",
    "            tAt = tAt + time.time()-t\n",
    "            \n",
    "            t = time.time()\n",
    "            tweet = remove_ends(tweet)\n",
    "            tEnds = tEnds + time.time()-t\n",
    "            \n",
    "            t = time.time()\n",
    "            tweet = remove_verb_contractions(tweet)\n",
    "            tVerb = tVerb + time.time()-t\n",
    "            \n",
    "            \n",
    "            t = time.time()\n",
    "            tweet = correctSlang(tweet,slangDict)\n",
    "            tSlang = tSlang + time.time()-t\n",
    "            \n",
    "            print(tweet)\n",
    "            \n",
    "            # Now using context2vec and Edit distance\n",
    "        if len(tweet)!=0:\n",
    "            t = time.time()\n",
    "            tweet = use_dict2vec(tweet, c2v_model,wordDict,1,1,1,400)\n",
    "            tD2V = tD2V + time.time()-t\n",
    "            \n",
    "            t = time.time()\n",
    "            tweet = clean_homophones(tweet, c2v_model)\n",
    "            tHomo = tHomo + time.time()-t\n",
    "            \n",
    "        if len(tweet)!=0:\n",
    "#             normalized_text.append(tweet)            \n",
    "            tweet = word_tokenize(tweet)\n",
    "            print(tweet)\n",
    "            normalized_text.append(tweet)\n",
    "    \n",
    "        if i>0 and i%100000==0:\n",
    "            if verbose is True:\n",
    "                print(i)\n",
    "                print('Time to clear non ASCII characters: '+str(tASCII)+'s')\n",
    "                print('Time to clear RTs: '+str(tRT)+'s')\n",
    "                print('Time to clear URLs and HTML '+str(tURLs)+'s')\n",
    "                print('Time to clear Hashtags: '+str(tHash)+'s')\n",
    "                print('Time to clear ATs: '+str(tAt)+'s')\n",
    "                print('Time to remove Ends: '+str(tEnds)+'s')\n",
    "                print('Time to clean verbs and negation: '+str(tVerb)+'s')\n",
    "                print('Time to clean slang: '+str(tSlang)+'s')\n",
    "                print('Time to clean misspelled words: '+str(tD2V)+'s')\n",
    "                print('Time to clean common homophonic errors: '+str(tHomo)+'s')\n",
    "                \n",
    "            tASCII = 0\n",
    "            tRT = 0\n",
    "            tURLs = 0\n",
    "            tHash = 0\n",
    "            tAt = 0\n",
    "            tEnds = 0\n",
    "            tVerb = 0\n",
    "            tSlang = 0\n",
    "            tD2V = 0\n",
    "        \n",
    "    print(i)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We have not got to stop: 'regex is not going to fail'. We have understood that they have figured we cannot avoid using it to solve 'verbs'. We are convinced it works. They are not and want to tell us to stop, but we are still going to\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"We haven't gotta stop: 'regex isn't gonna fail'. We've understood that they've figured we can't avoid using it to solve 'verbs'. We're convinced it works. They aren't and wanna tell us to stop, but we're still gonna\"\n",
    "remove_verb_contractions(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 's disappointing how people nowadays think terrorism is linked to a religion\n",
      "['It', 'is', 'disappointing', 'how', 'people', 'nowadays', 'think', 'terrorism', 'is', 'linked', 'to', 'a', 'religion']\n",
      "Israel killing Muslims everyday and no one bats an eye. Terrorist attack and Muslims got the blame? How shallow can you be\n",
      "['Israel', 'killing', 'Muslims', 'everyday', 'and', 'no', 'one', 'bats', 'an', 'eye', '.', 'Terrorist', 'attack', 'and', 'Muslims', 'got', 'the', 'blame', '?', 'How', 'shallow', 'can', 'you', 'be']\n",
      "that bitch stops a show because someone spilled water on stage but puts on a show when terrorists attacki\n",
      "['that', 'bitch', 'stops', 'a', 'show', 'because', 'someone', 'spilled', 'water', 'on', 'stage', 'but', 'puts', 'on', 'a', 'show', 'when', 'terrorist', 'attacks']\n",
      "French President Francois Hollande condemns the \"terrorist attacks of unprecedented proportions.\"  #Pray-For-Paris\n",
      "['French', 'President', 'Francois', 'Hollande', 'condemns', 'the', '``', 'terrorist', 'attacks', 'on', 'unprecedented', 'proportions', '.', \"''\", '#', 'Pray-For-Paris']\n",
      "NY lights  in blue, white and red as we stand in solidarity with the people of France\n",
      "['NY', 'lights', 'in', 'blue', ',', 'white', 'and', 'red', 'as', 'we', 'stand', 'in', 'solidarity', 'with', 'the', 'people', 'of', 'France']\n",
      "My name is Malik Riaz. I am a Muslim. I condemn the  #Paris-Attack. Over 1.5 billion Muslims do\n",
      "['My', 'name', 'is', 'Malik', 'Riaz', '.', 'I', 'am', 'a', 'Muslim', '.', 'I', 'condemn', 'the', '#', 'Paris-Attack', '.', 'Over', '1.5', 'billion', 'Muslims', 'do']\n",
      "Please remember this\n",
      "['Please', 'remember', 'this']\n",
      "Good on you, Paris\n",
      "['Good', 'on', 'you', ',', 'Paris']\n",
      "Facebook Is Asking People In Paris To Check In If They are Safe\n",
      "['Facebook', 'Is', 'Asking', 'People', 'In', 'Paris', 'To', 'Check', 'In', 'If', 'They', 'are', 'Safe']\n",
      "9\n",
      "14.4138150215\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "norm_tweets = normalize_tweet_text(text_full[:10],c2v_model,large_dict,dict_abbreviation,False)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not a vonpetition\n",
      "['This', 'is', 'not', 'a', 'contradiction']\n",
      "thsi is amzing! How old is you\n",
      "['this', 'is', 'amazing', '!', 'How', 'old', 'is', 'you']\n",
      "I do not know. wehre to go\n",
      "['I', 'do', 'not', 'know', '.', 'were', 'you', 'go']\n",
      "I really think do your very stupid\n",
      "['I', 'really', 'think', 'do', 'your', 'very', 'stupid']\n",
      "I could not remember the write answer\n",
      "['I', 'could', 'not', 'remember', 'to', 'write', 'answer']\n",
      "I personnaly think it 'd be easier like this\n",
      "['I', 'personaly', 'think', 'it', 'woudl', 'be', 'easier', 'like', 'this']\n",
      "It 's going to take you forever to come bcak home\n",
      "['It', 'is', 'going', 'to', 'take', 'you', 'forever', 'to', 'come', 'back', 'home']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# text_extract = norm_tweets[:10000]\n",
    "# text_extract = ['The KKK was screaming \"kill all n*ggers\" 2 DAYS AGO', 'They are not responsib']\n",
    "text_extract = ['This is not a vonpetition !',\n",
    "                'thsi is amzing! How old is you?',\n",
    "                \"I don't know. wehre to go\",\n",
    "                \"I really think do your very stupid\",\n",
    "                \"I couldn't remember the write answer\",\n",
    "                \"I personnaly think it'd be easier like this\",\n",
    "                \"It's gonna take you forever to come bcak home\"]\n",
    "norm_text = normalize_tweet_text(text_extract,c2v_model,large_dict,dict_abbreviation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "text_extract = norm_tweets[:1000]\n",
    "# print(text_extract)\n",
    "tweet_out = []\n",
    "for i,tweet in enumerate(text_extract):\n",
    "    print(tweet)\n",
    "#     parsed_tweet = re.findall(r\"[#\\w\\'*]+\", tweet)\n",
    "    split = split = re.split(\"-|;|:|\\(|\\)|\\[|\\]|\\.| |,|!|\\~|\\?\",tweet)\n",
    "    parsed_tweet = [s for s in split if len(s)!=0]\n",
    "    if len(parsed_tweet)>1:\n",
    "        for word in parsed_tweet:   \n",
    "            if word[0].isalpha() and word[0].islower():\n",
    "                dict_letter = large_dict[alphabet_dict[word[0].lower()]]\n",
    "                \n",
    "                if word.lower() not in dict_letter:\n",
    "                    i_start = 0\n",
    "                    stop = False\n",
    "                    while stop is False:\n",
    "                        idx = tweet.find(word,i_start)\n",
    "                        if idx>0 and idx<len(tweet)-len(word):\n",
    "                            if not tweet[idx-1].isalpha() and not tweet[idx+len(word)].isalpha():\n",
    "                                #                                 tweet = tweet.replace(word,' [] ',1)\n",
    "                                tweet = tweet[:idx]+' [] '+tweet[idx+len(word):]\n",
    "                                eval_vec = evalc2v(tweet,c2v_model,False,100)\n",
    "                                stop=True\n",
    "                            else:\n",
    "                                i_start = idx+1\n",
    "                        elif idx==0 and len(tweet)>1:\n",
    "#                             print(len(tweet))\n",
    "#                             print(idx+len(word))\n",
    "                            if not tweet[idx+len(word)].isalpha():\n",
    "                                #                                 tweet = tweet.replace(word,' [] ',1)\n",
    "                                tweet = tweet[:idx]+' [] '+tweet[idx+len(word):]\n",
    "                                eval_vec = evalc2v(tweet,c2v_model,False,100)\n",
    "                                stop=True\n",
    "                            else:\n",
    "                                i_start = idx+1\n",
    "                                \n",
    "                        elif idx==len(tweet)-len(word):\n",
    "                            if not tweet[idx-1].isalpha():\n",
    "                                #                                 tweet = tweet.replace(word, ' [] ',1)\n",
    "                                tweet = tweet[:idx]+' [] '+tweet[idx+len(word):]\n",
    "                                eval_vec = evalc2v(tweet,c2v_model,False,100)\n",
    "                                stop=True\n",
    "                            else:\n",
    "                                i_start = idx+1\n",
    "                        else:\n",
    "                            i_start = idx+1\n",
    "                            \n",
    "                    print(tweet)\n",
    "                    dist = 100\n",
    "                    idx_min = 0\n",
    "                    \n",
    "                    candidates = [e[0] for e in eval_vec if e[0].isalpha()]+dict_letter\n",
    "                    \n",
    "                    for i,candidate in enumerate(candidates):\n",
    "                        if candidate[0].isalpha():\n",
    "                            dist_temp = computeEditDistance(candidate.lower(),word.lower(),1.5,0.5,1)\n",
    "#                             computeEditDistance(str1, str2,w_del=1,w_ins=1,w_sub=1):\n",
    "                            if dist_temp<dist:\n",
    "                                dist=dist_temp\n",
    "                                idx_min = i\n",
    "                                \n",
    "                    tweet = tweet.replace(' [] ', candidates[idx_min],1)\n",
    "                        #             eval_vec = evalc2v(tweet,c2v_model)\n",
    "                        #             tweet_out = tweet.replace(word, eval_vec[0])\n",
    "                        \n",
    "            elif word.lower()==\"'s\":\n",
    "                i_start = 0\n",
    "                stop = False\n",
    "                while stop is False:\n",
    "                    idx = tweet.find(word,i_start)\n",
    "                    if idx>0 and idx<len(tweet)-len(word):\n",
    "                        if not tweet[idx-1].isalpha() and not tweet[idx+len(word)].isalpha():\n",
    "                            #                                 tweet = tweet.replace(word,' [] ',1)\n",
    "                            tweet = tweet[:idx]+' [] '+tweet[idx+len(word):]\n",
    "                            eval_vec = evalc2v(tweet,c2v_model,False,100)\n",
    "                            stop=True\n",
    "                        else:\n",
    "                            i_start = idx+1\n",
    "                    elif idx==0 and len(tweet)>1:\n",
    "                        #                             print(len(tweet))\n",
    "                        #                             print(idx+len(word))\n",
    "                        if not tweet[idx+len(word)].isalpha():\n",
    "                            #                                 tweet = tweet.replace(word,' [] ',1)\n",
    "                            tweet = tweet[:idx]+' [] '+tweet[idx+len(word):]\n",
    "                            eval_vec = evalc2v(tweet,c2v_model,False,100)\n",
    "                            stop=True\n",
    "                        else:\n",
    "                            i_start = idx+1\n",
    "                            \n",
    "                    elif idx==len(tweet)-len(word):\n",
    "                        if not tweet[idx-1].isalpha():\n",
    "                            #                                 tweet = tweet.replace(word, ' [] ',1)\n",
    "                            tweet = tweet[:idx]+' [] '+tweet[idx+len(word):]\n",
    "                            eval_vec = evalc2v(tweet,c2v_model,False,100)\n",
    "                            stop=True\n",
    "                        else:\n",
    "                            i_start = idx+1\n",
    "                    else:\n",
    "                        i_start = idx+1\n",
    "                        \n",
    "                print(tweet)\n",
    "                dist = 100\n",
    "                idx_min = 0\n",
    "                \n",
    "                candidates = [e[0] for e in eval_vec if e[0].isalpha()]\n",
    "                \n",
    "                for i,candidate in enumerate(candidates):\n",
    "                    if candidate[0].isalpha():\n",
    "                        dist_temp = computeEditDistance(candidate.lower(),word.lower(),1.5,0.5,1)\n",
    "                        #                             computeEditDistance(str1, str2,w_del=1,w_ins=1,w_sub=1):\n",
    "                        if dist_temp<dist:\n",
    "                            dist=dist_temp\n",
    "                            idx_min = i\n",
    "                            \n",
    "                tweet = tweet.replace(' [] ', candidates[idx_min],1)\n",
    "                        \n",
    "    else:\n",
    "        if tweet[0].isalpha() and tweet[0].islower():\n",
    "            dict_letter = large_dict[alphabet_dict[tweet[0].lower()]]\n",
    "            if tweet not in dict_letter:\n",
    "                dist = 100\n",
    "                idx_min = 0\n",
    "                for i, candidate in enumerate(dict_letter):\n",
    "                    dist_temp = computeEditDistance(candidate.lower(),tweet.lower(),1.5,0.5,1)\n",
    "#                     computeEditDistance(str1, str2,w_del=1,w_ins=1,w_sub=1):\n",
    "                    if dist_temp<dist:\n",
    "                        dist=dist_temp\n",
    "                        idx_min = i\n",
    "                        \n",
    "                tweet = dict_letter[idx_min]\n",
    "            \n",
    "    print(tweet)\n",
    "\n",
    "print(time.time()-t)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO change levenshtein weights for last words, allow more insertion, allow apostrophes in words, magnitud --> manitus, transform it's in it 's (with remove_contractions) then split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #BREAKING-NeWS'"
      ]
     },
     "execution_count": 1499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag = '#BREAKING-NeWS'\n",
    "parseHashtags(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 NLP",
   "language": "python",
   "name": "py2nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
